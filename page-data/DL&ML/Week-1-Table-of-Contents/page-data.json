{"componentChunkName":"component---src-templates-blog-post-js","path":"/DL&ML/Week-1-Table-of-Contents/","result":{"data":{"site":{"siteMetadata":{"author":"Young Jin Ahn","comment":{"utterances":"snoop2head/snoop2head.github.io"},"sponsor":{"buyMeACoffeeId":""}}},"markdownRemark":{"excerpt":"References Naver AI Boostcamp Statistics 110 by Joseph Blitzstein Linear Algebra by Gilbert Strang Linear Algebra by 3Blue1Brown Grokking Deep Learning Dive into Deep Learning Mathematics Appendix Maâ€¦","html":"<h3 id=\"references\" style=\"position:relative;\"><a href=\"#references\" aria-label=\"references permalink\" class=\"toc-header before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>References</h3>\n<ul>\n<li><a href=\"https://www.boostcourse.org/boostcampaitech2/joinLectures/283128?isDesc=false\">Naver AI Boostcamp</a></li>\n<li><a href=\"https://www.youtube.com/watch?v=KbB0FjPg0mw&#x26;list=PL2SOU6wwxB0uwwH80KTQ6ht66KWxbzTIo\">Statistics 110 by Joseph Blitzstein</a></li>\n<li><a href=\"https://www.youtube.com/watch?v=QVKj3LADCnA&#x26;list=PL49CF3715CB9EF31D\">Linear Algebra by Gilbert Strang</a></li>\n<li><a href=\"https://www.youtube.com/watch?v=fNk_zzaMoSs&#x26;list=PLZHQObOWTQDPD3MizzM2xVFitgF8hE_ab\">Linear Algebra by 3Blue1Brown</a></li>\n<li><a href=\"https://github.com/iamtrask/Grokking-Deep-Learning\">Grokking Deep Learning</a></li>\n<li><a href=\"https://d2l.ai/chapter_appendix-mathematics-for-deep-learning/index.html\">Dive into Deep Learning Mathematics Appendix</a></li>\n<li><a href=\"https://mml-book.github.io/\">Mathematics for Machine Learning</a></li>\n</ul>\n<h3 id=\"lecture-1--vectors\" style=\"position:relative;\"><a href=\"#lecture-1--vectors\" aria-label=\"lecture 1  vectors permalink\" class=\"toc-header before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Lecture 1 | Vectors</h3>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 702px; \"\n    >\n      <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 78.33333333333333%; position: relative; bottom: 0; left: 0; background-image: url('data:image/jpeg;base64,/9j/2wBDABALDA4MChAODQ4SERATGCgaGBYWGDEjJR0oOjM9PDkzODdASFxOQERXRTc4UG1RV19iZ2hnPk1xeXBkeFxlZ2P/2wBDARESEhgVGC8aGi9jQjhCY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2P/wgARCAAQABQDASIAAhEBAxEB/8QAFwABAAMAAAAAAAAAAAAAAAAAAAECBf/EABQBAQAAAAAAAAAAAAAAAAAAAAD/2gAMAwEAAhADEAAAAdsFkj//xAAYEAEAAwEAAAAAAAAAAAAAAAAAARARQf/aAAgBAQABBQJiK6//xAAUEQEAAAAAAAAAAAAAAAAAAAAQ/9oACAEDAQE/AT//xAAUEQEAAAAAAAAAAAAAAAAAAAAQ/9oACAECAQE/AT//xAAUEAEAAAAAAAAAAAAAAAAAAAAg/9oACAEBAAY/Al//xAAYEAADAQEAAAAAAAAAAAAAAAAAARExgf/aAAgBAQABPyGVqmpnTA1lTHgR/9oADAMBAAIAAwAAABATz//EABQRAQAAAAAAAAAAAAAAAAAAABD/2gAIAQMBAT8QP//EABQRAQAAAAAAAAAAAAAAAAAAABD/2gAIAQIBAT8QP//EABoQAQADAQEBAAAAAAAAAAAAAAEAESExYVH/2gAIAQEAAT8QST1ewGAg+9ewGKSmte+zTEt5cI0DQSgYz//Z'); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"img\"\n        title=\"img\"\n        src=\"/static/ac89bbb33ed59e38c3e4b98fe4424210/eafa3/2355F4475674FB4208.jpg\"\n        srcset=\"/static/ac89bbb33ed59e38c3e4b98fe4424210/f93b5/2355F4475674FB4208.jpg 300w,\n/static/ac89bbb33ed59e38c3e4b98fe4424210/b4294/2355F4475674FB4208.jpg 600w,\n/static/ac89bbb33ed59e38c3e4b98fe4424210/eafa3/2355F4475674FB4208.jpg 702w\"\n        sizes=\"(max-width: 702px) 100vw, 702px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n      />\n    </span></p>\n<p><a href=\"https://rfriend.tistory.com/145\">Image Reference</a></p>\n<ul>\n<li>What is Vector</li>\n<li>Distance between two vectors</li>\n<li>Angle between two vectors</li>\n<li>\n<p>Vector's Norm</p>\n<ul>\n<li>L1 Norm</li>\n<li>L2 Norm</li>\n</ul>\n</li>\n<li>\n<p>Vector's Dot Product</p>\n<ul>\n<li>same as Inner Product, Dot Product, Scalar Product, projection product</li>\n</ul>\n</li>\n<li>Vector's Orthogonality</li>\n<li>Vector's Projection</li>\n</ul>\n<h3 id=\"lecture-2--linear-algebra\" style=\"position:relative;\"><a href=\"#lecture-2--linear-algebra\" aria-label=\"lecture 2  linear algebra permalink\" class=\"toc-header before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Lecture 2 | Linear Algebra</h3>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 585px; \"\n    >\n      <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 66.33333333333333%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAANCAIAAAAmMtkJAAAACXBIWXMAAAsTAAALEwEAmpwYAAABS0lEQVQoz4WS2YqDUAyGz/u/lzeCV14JQqm7dd+XVjufJ0UchpkJtI1J/iWxqu/7qqrSNM3znITvLMuSJFmW5f170PV9X5GBf71e4zjO8/z+M57P5zAMTLZtC+QAo1aWJQ34IKKy7/tPGBiQtBjDHUoHmB96MgTfNE1XmBCBlEVYLQxDHskVUqJpGAZ513VN05wYWpyDIgl1eC3LchwH8LZtal1XMgSZIMFPqiPXwTpRFLmuywmDIHg8HsgAwyzDSuihRIqVeOQYUGAPxlN/04ESlVHHoUwPTUTiOOYZYsGQg5EToINnlkLc8zw56rEzn0kHE7w6KMkxyVUYIhdfYORO4u4bmImiKG63G0tSQRZNmRCAaZq2bUN3/S8o5gCDueuQs11flXjhECRYq+sayAeMJibhFko5/hXMKJVdByyno4/tcwjZ60r/xhd/EvCt/PbAcAAAAABJRU5ErkJggg=='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"Lang&#39;s Linear algebra or Introduction to linear algebra for an  undergraduate - Mathematics Educators Stack Exchange\"\n        title=\"Lang&#39;s Linear algebra or Introduction to linear algebra for an  undergraduate - Mathematics Educators Stack Exchange\"\n        src=\"/static/203afdb759df7e257b8d9917f505d7c2/78a22/dfZND.png\"\n        srcset=\"/static/203afdb759df7e257b8d9917f505d7c2/5a46d/dfZND.png 300w,\n/static/203afdb759df7e257b8d9917f505d7c2/78a22/dfZND.png 585w\"\n        sizes=\"(max-width: 585px) 100vw, 585px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n      />\n    </span></p>\n<ul>\n<li>What is Matrix</li>\n<li>Matrix Operations: Addition, Multiplication</li>\n<li>Spaces of Vectors</li>\n<li>Inverse Matrices</li>\n<li>Pseudo-inverse matrices (same as Moore-Penrose Matrix)</li>\n</ul>\n<h3 id=\"lecture-34--gradient-descent\" style=\"position:relative;\"><a href=\"#lecture-34--gradient-descent\" aria-label=\"lecture 34  gradient descent permalink\" class=\"toc-header before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Lecture 3,4 | Gradient Descent</h3>\n<ul>\n<li>Differentiation</li>\n<li>Gradient Ascent &#x26; Gradient Descent</li>\n<li>Gradient Descent in Code</li>\n<li>Nabla &#x26; Partial Differentiation for vectors</li>\n<li>Gradient Vectors</li>\n<li>Linear Regression and pseudoinverse</li>\n<li>Getting beta's minimum using linear regression gradient descent</li>\n<li>Limitations of Gradient Descent</li>\n</ul>\n<h3 id=\"lecture-4--stochastic-gradient-descentsgd\" style=\"position:relative;\"><a href=\"#lecture-4--stochastic-gradient-descentsgd\" aria-label=\"lecture 4  stochastic gradient descentsgd permalink\" class=\"toc-header before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Lecture 4 | Stochastic Gradient Descent(SGD)</h3>\n<ul>\n<li>Stochastic Gradient Descent for non-convex functions</li>\n<li>Minibatch SGD</li>\n</ul>\n<h3 id=\"lecture-5--deep-learning\" style=\"position:relative;\"><a href=\"#lecture-5--deep-learning\" aria-label=\"lecture 5  deep learning permalink\" class=\"toc-header before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Lecture 5 | Deep Learning</h3>\n<ul>\n<li>Non-linear neural network</li>\n<li>softmax</li>\n<li>activation function - sigmoid, tanh(<strong>hyperbolic tangent</strong>), ReLU</li>\n<li>Multi-layer perceptron</li>\n<li>Forward Propogation</li>\n<li>Backpropogation</li>\n<li>Gradient vectors and chain rules</li>\n</ul>\n<h3 id=\"lecture-6--probabilities\" style=\"position:relative;\"><a href=\"#lecture-6--probabilities\" aria-label=\"lecture 6  probabilities permalink\" class=\"toc-header before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Lecture 6 | Probabilities</h3>\n<ul>\n<li>Loss function</li>\n<li>Probability distributions</li>\n<li>Discrete Random Variable, Probability Mass Function</li>\n<li>Continuous Random Variable, Probability Distribution Function</li>\n<li>Conditional Probability</li>\n<li>Expectations</li>\n<li>Variance, Covariance, SKewness</li>\n<li>Monte Carlo Sampling Method</li>\n</ul>\n<h3 id=\"lecture-7--statistical-inference\" style=\"position:relative;\"><a href=\"#lecture-7--statistical-inference\" aria-label=\"lecture 7  statistical inference permalink\" class=\"toc-header before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Lecture 7 | Statistical Inference</h3>\n<ul>\n<li>Probability Distribution</li>\n<li>Sample Mean, Sample Variance</li>\n<li>Sampling Distribution</li>\n<li>Maximum Likelihood Estimation(MLE)</li>\n<li>Log-likelihood, Negative log-likelihood</li>\n<li>Sample MLE</li>\n<li>MLE Estimation</li>\n<li>KL Divergence</li>\n<li>Cross Entropy</li>\n<li>Entropy</li>\n</ul>\n<h3 id=\"lecture-8--bayesian-statistics\" style=\"position:relative;\"><a href=\"#lecture-8--bayesian-statistics\" aria-label=\"lecture 8  bayesian statistics permalink\" class=\"toc-header before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Lecture 8 | Bayesian Statistics</h3>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 1200px; \"\n    >\n      <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 54.333333333333336%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAALCAYAAAB/Ca1DAAAACXBIWXMAABYlAAAWJQFJUiTwAAACE0lEQVQoz3VS2W7bMBDU/39MX1ogQI88BAmKOE6d1HXTWId12bIj2TpInSQ1WVJw4T50oZHIJbUze1hZpuC4Ao4jzPf2VmDjCxwOI2wb8H2JcZQQQkJKvR6hTSll9pfQZ5aUI+paoOskQaCqBnTtAG8TYUNo6p5+FlBSoG1bRNGWSHyUZUV+CiQEhmEwGInEatsGQeDjeMxwtpRRwO0JL16Ct6rHiQtk5EuZQHBgsMMUh3IwvoLEXJqVpiml5sAPQmy3CSq6MPdKPPoM8w3DzC3wSPuZc8L1Dxcfb55x9X2FLw9r3NsZfsdE4HgIwwhRvIOlKPe8ZCj4gLKWeCt6PLglnqMaq6TFMuZY0U/LiOHJL/CVAl3dLTF3MvyMONY7jlNeoqASlBWHpWXWTTexBAG8IMH1wseHb3N8unnC59kr4rRGNyg0vQQ9Bq3eU915K/9NWb+EUCZ6RchZS8wM968J7lYRpZsjo0b9z4RU6MUI3XzqCSzd6vMoaGt6hUXA8GvXUsodFiHHoejMmR6NijHkRWG6qq1iDVzPp8aGNG7BpHCyKahmc/c17G2JdVzQmlTz4e+d86icRfSDRFHV4HVjYDHNSE3Ja2LjLTHTXFEJkmRvWLuuN7XSqemZ1WkpiiWVXo8kQGG8rKHvb/DHDvASS+yPDQXsiFUQBvT9AM5rMK4VTCq0z5yZO9M9QYo19OC/A/2wRqoXQdTSAAAAAElFTkSuQmCC'); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"image-20210808141145127\"\n        title=\"image-20210808141145127\"\n        src=\"/static/dc23d4c5b5ee7ea37146e2f4cce24705/c1b63/image-20210808141145127.png\"\n        srcset=\"/static/dc23d4c5b5ee7ea37146e2f4cce24705/5a46d/image-20210808141145127.png 300w,\n/static/dc23d4c5b5ee7ea37146e2f4cce24705/0a47e/image-20210808141145127.png 600w,\n/static/dc23d4c5b5ee7ea37146e2f4cce24705/c1b63/image-20210808141145127.png 1200w,\n/static/dc23d4c5b5ee7ea37146e2f4cce24705/b1ffc/image-20210808141145127.png 1492w\"\n        sizes=\"(max-width: 1200px) 100vw, 1200px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n      />\n    </span></p>\n<ul>\n<li>Posterial, Prior, Evidence, Likelihood</li>\n<li>True Positive and Recall Rate</li>\n<li>False Negative</li>\n<li>True Negative and Specificty</li>\n<li>False Positive</li>\n<li>Precision</li>\n</ul>\n<h3 id=\"lecture-9--convolutional-neural-networkcnn\" style=\"position:relative;\"><a href=\"#lecture-9--convolutional-neural-networkcnn\" aria-label=\"lecture 9  convolutional neural networkcnn permalink\" class=\"toc-header before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Lecture 9 | Convolutional Neural Network(CNN)</h3>\n<ul>\n<li>MLP vs CNN</li>\n<li>Kernal(same as Weight Matrix, Filters, Window)</li>\n<li>1D Conv</li>\n<li>2D Conv</li>\n<li>\n<p>3D Conv</p>\n<ul>\n<li>3D Conv are 2D conv stacked up as 3 Channels</li>\n<li>Tensors are stacked matrices as much as 3 Channels</li>\n</ul>\n</li>\n<li>Backpropogation in Convolution</li>\n</ul>\n<h3 id=\"lecture-10--recurrent-neural-networkrnn\" style=\"position:relative;\"><a href=\"#lecture-10--recurrent-neural-networkrnn\" aria-label=\"lecture 10  recurrent neural networkrnn permalink\" class=\"toc-header before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Lecture 10 | Recurrent Neural Network(RNN)</h3>\n<ul>\n<li>Sequential Data</li>\n<li>Conditional Probability</li>\n<li>Backpropagation Through Time (BPTT)</li>\n<li>Truncated BPTT</li>\n<li>LSTM, GRU</li>\n</ul>\n<style class=\"grvsc-styles\">\n  .grvsc-container {\n    overflow: auto;\n    position: relative;\n    -webkit-overflow-scrolling: touch;\n    padding-top: 1rem;\n    padding-top: var(--grvsc-padding-top, var(--grvsc-padding-v, 1rem));\n    padding-bottom: 1rem;\n    padding-bottom: var(--grvsc-padding-bottom, var(--grvsc-padding-v, 1rem));\n    border-radius: 8px;\n    border-radius: var(--grvsc-border-radius, 8px);\n    font-feature-settings: normal;\n    line-height: 1.4;\n  }\n  \n  .grvsc-code {\n    display: table;\n  }\n  \n  .grvsc-line {\n    display: table-row;\n    box-sizing: border-box;\n    width: 100%;\n    position: relative;\n  }\n  \n  .grvsc-line > * {\n    position: relative;\n  }\n  \n  .grvsc-gutter-pad {\n    display: table-cell;\n    padding-left: 0.75rem;\n    padding-left: calc(var(--grvsc-padding-left, var(--grvsc-padding-h, 1.5rem)) / 2);\n  }\n  \n  .grvsc-gutter {\n    display: table-cell;\n    -webkit-user-select: none;\n    -moz-user-select: none;\n    user-select: none;\n  }\n  \n  .grvsc-gutter::before {\n    content: attr(data-content);\n  }\n  \n  .grvsc-source {\n    display: table-cell;\n    padding-left: 1.5rem;\n    padding-left: var(--grvsc-padding-left, var(--grvsc-padding-h, 1.5rem));\n    padding-right: 1.5rem;\n    padding-right: var(--grvsc-padding-right, var(--grvsc-padding-h, 1.5rem));\n  }\n  \n  .grvsc-source:empty::after {\n    content: ' ';\n    -webkit-user-select: none;\n    -moz-user-select: none;\n    user-select: none;\n  }\n  \n  .grvsc-gutter + .grvsc-source {\n    padding-left: 0.75rem;\n    padding-left: calc(var(--grvsc-padding-left, var(--grvsc-padding-h, 1.5rem)) / 2);\n  }\n  \n  /* Line transformer styles */\n  \n  .grvsc-has-line-highlighting > .grvsc-code > .grvsc-line::before {\n    content: ' ';\n    position: absolute;\n    width: 100%;\n  }\n  \n  .grvsc-line-diff-add::before {\n    background-color: var(--grvsc-line-diff-add-background-color, rgba(0, 255, 60, 0.2));\n  }\n  \n  .grvsc-line-diff-del::before {\n    background-color: var(--grvsc-line-diff-del-background-color, rgba(255, 0, 20, 0.2));\n  }\n  \n  .grvsc-line-number {\n    padding: 0 2px;\n    text-align: right;\n    opacity: 0.7;\n  }\n  \n</style>","tableOfContents":"<ul>\n<li><a href=\"/DL&#x26;ML/Week-1-Table-of-Contents/#references\">References</a></li>\n<li><a href=\"/DL&#x26;ML/Week-1-Table-of-Contents/#lecture-1--vectors\">Lecture 1 | Vectors</a></li>\n<li><a href=\"/DL&#x26;ML/Week-1-Table-of-Contents/#lecture-2--linear-algebra\">Lecture 2 | Linear Algebra</a></li>\n<li><a href=\"/DL&#x26;ML/Week-1-Table-of-Contents/#lecture-34--gradient-descent\">Lecture 3,4 | Gradient Descent</a></li>\n<li><a href=\"/DL&#x26;ML/Week-1-Table-of-Contents/#lecture-4--stochastic-gradient-descentsgd\">Lecture 4 | Stochastic Gradient Descent(SGD)</a></li>\n<li><a href=\"/DL&#x26;ML/Week-1-Table-of-Contents/#lecture-5--deep-learning\">Lecture 5 | Deep Learning</a></li>\n<li><a href=\"/DL&#x26;ML/Week-1-Table-of-Contents/#lecture-6--probabilities\">Lecture 6 | Probabilities</a></li>\n<li><a href=\"/DL&#x26;ML/Week-1-Table-of-Contents/#lecture-7--statistical-inference\">Lecture 7 | Statistical Inference</a></li>\n<li><a href=\"/DL&#x26;ML/Week-1-Table-of-Contents/#lecture-8--bayesian-statistics\">Lecture 8 | Bayesian Statistics</a></li>\n<li><a href=\"/DL&#x26;ML/Week-1-Table-of-Contents/#lecture-9--convolutional-neural-networkcnn\">Lecture 9 | Convolutional Neural Network(CNN)</a></li>\n<li><a href=\"/DL&#x26;ML/Week-1-Table-of-Contents/#lecture-10--recurrent-neural-networkrnn\">Lecture 10 | Recurrent Neural Network(RNN)</a></li>\n</ul>","frontmatter":{"date":"2021-08-07","title":"Week 1 Table of Contents","tags":["Theory"]}}},"pageContext":{"slug":"/DL&ML/Week-1-Table-of-Contents/","previous":{"fields":{"slug":"/DL&ML/Professor-Lim's-Master-Class/"},"frontmatter":{"title":"Professor-Lim's-Master-Class(KOR)"}},"next":{"fields":{"slug":"/DL&ML/Gradient-Descent-on-1D-loss-function/"},"frontmatter":{"title":"Gradient Descent on 1D loss function"}}}},"staticQueryHashes":["1081905842","3911196313"]}