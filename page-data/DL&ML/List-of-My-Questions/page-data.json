{"componentChunkName":"component---src-templates-blog-post-js","path":"/DL&ML/List-of-My-Questions/","result":{"data":{"site":{"siteMetadata":{"author":"Young Jin Ahn","comment":{"utterances":"snoop2head/snoop2head.github.io"},"sponsor":{"buyMeACoffeeId":""}}},"markdownRemark":{"excerpt":"Collection of questions of myself and my teammates' captures critical concepts for learning. While learning from certain study materials, usually there are numerous questions that arise in my head. D…","html":"<p><strong>Collection of questions of myself and my teammates' captures critical concepts for learning.</strong></p>\n<p>While learning from certain study materials, usually there are numerous questions that arise in my head. Due to limitation of the time or lack of interest, only selected few questions are answered within my timespan. However, even if I think that I know for sure, unanswered questions will one day cause trouble. Attempting to answer other people's questions help to filter out possibly vauge knowledge of mine.</p>\n<h2 id=\"questions-during-pytorch-study\" style=\"position:relative;\"><a href=\"#questions-during-pytorch-study\" aria-label=\"questions during pytorch study permalink\" class=\"toc-header before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Questions during Pytorch Study</h2>\n<p><a href=\"https://github.com/Boostcamp-AI-Tech-Team123/meetup-log/wiki/2021-08-11\">https://github.com/Boostcamp-AI-Tech-Team123/meetup-log/wiki/2021-08-11</a></p>\n<ul>\n<li>\n<p>Why does the class and attributes has to be reinitialized?</p>\n<ul>\n<li>Initializing 왜 하는 지보다는 Initializing을 어떻게 해야 좋은지라는 질문이 더 적합한 것 같다. 처음에 모델을 학습할 때는 정답인 parameter 값을 모르기 때문에, 필연적으로 어떠한 값으로 초기값을 부여해줘야 한다. 그렇다면 <strong>어떻게 initializing을 해야 효과적으로 학습을 진행할 수 있는지</strong>가 더 중요한 것 같다.</li>\n<li><a href=\"https://www.deeplearning.ai/ai-notes/initialization/\">https://www.deeplearning.ai/ai-notes/initialization/</a></li>\n<li><a href=\"https://stackoverflow.com/questions/63058355/why-is-the-super-constructor-necessary-in-pytorch-custom-modules\">https://stackoverflow.com/questions/63058355/why-is-the-super-constructor-necessary-in-pytorch-custom-modules</a></li>\n</ul>\n</li>\n<li>\n<p>Why is self.variable = variable needed?</p>\n<ul>\n<li>self는 클래스의 인스턴스를 나타내는 변수이다. 반면 init의 parameter로 넘어온 ksize는 함수에서만 쓰이는 지역변수이다. 이런 개념을 생각했을 때 ksize대신 self.ksize로 쓰는 것이 올바른 것 같다.</li>\n<li>init함수에서는 파라미터로 ksize를 받고 이를 self.ksize에 값을 할당해주고 있다. 따라서 init함수에서는 self.ksize나 ksize가 같은 값이므로 self.ksize대신 ksize를 써도 무방할 듯 하다.</li>\n<li>init이 아닌 다른 함수에서는 ksize를 쓰고 싶다면 self.ksize을 이용해야 한다.</li>\n</ul>\n</li>\n<li>How the numbers of input channel and output channel is decided?</li>\n<li>When should we use activiation function and pooling when adding layers for the transfer learning?</li>\n<li>How is transformers look like?</li>\n<li>\n<p>What exactly is gradient descent? Any illustrations?</p>\n<ul>\n<li><a href=\"https://www.deeplearning.ai/ai-notes/optimization/\">https://www.deeplearning.ai/ai-notes/optimization/</a></li>\n</ul>\n</li>\n<li>\n<p>How does torch.gather(input_tensor, dim, index) work??</p>\n<ul>\n<li><a href=\"https://stackoverflow.com/questions/50999977/what-does-the-gather-function-do-in-pytorch-in-layman-terms/51032153#51032153\">https://stackoverflow.com/questions/50999977/what-does-the-gather-function-do-in-pytorch-in-layman-terms/51032153#51032153</a></li>\n</ul>\n</li>\n<li>\n<p>Why are Optimizer has to be initialized?</p>\n<ul>\n<li>Because it saves gradient on the buffer</li>\n</ul>\n</li>\n<li>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 596px; \"\n    >\n      <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 128%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAaCAYAAAC3g3x9AAAACXBIWXMAAAsTAAALEwEAmpwYAAADz0lEQVRIx31Vy27bVhTUJ3TRTYvUbmy9KVEkRZGiKJKiZFmR/FCc2EGaPlIECIqiKNB10fYL+gPd9Be6789NZ65IRrHsLg4uL6/u8MycOUe1046Pp+1hFfWub+KkMzSh84b1Ya+zJvet3qha96MWBRHm0wkjxiKJEY7GGPljZJOJiSiMYDuB+Y32OuvaD4MZwMt5gtfrDDfLFF9tZjgj6IQgb/j8zeUM51mMJJqY59tVhqtFYj7a6D4CqC+J1omoFXQaRZT0S3r6XXnx0Qx1sV1sdjrp4k67dv/wsp7vg+3va64XosuLonr3LMM6T7BeZlhmU4yDcQVwP8yHHyhQzSoE7g8ChCxQPArxxnUR+6HJUNnrNz2ed7jvFu9ULJ8FOsiw3Ei/vhvC7Xj4/dMnCLoutucZnp8leM6CXbF4L89TrJi5ZQd4e5WbYu4Daq1VWkhL/tCxhnh/1EDUH2LDigbMQg7YEliO0LOyVbG03textv/Cpp4eAX/5/CnGBLw8S+G4AV5R200+NdmVGQrQuKHrVxb6KENDmYBDAv762THC3g7w3TaHNwyN/74jzQtm6VAaFVAF/fYyx9cXM6OtASzR9cIiBbvnY1vvwCFgTEMvWf0hAdUh6haZWlQX7Cx1jJygDqoy9ArbnKcxXm9omyzBbZ7j1WpmNFMkUWQoHrd2/SyqXzS9qtdloUpDfWVH24fvjmFFPrw/JrCzkdmHzEAZliG6uvNsNkXKzNr3DP7BNgS0+yGasYcv/+6iMR/gRc7eZXWln4qiCi/JRH68Y18r2o/ZpknAnhWgEbuo/+XCWnrYzlJTEImuy7KOVjWB6Je93tjroCpDATp2iM6Euvw5QCd3cZ2lGLEIF4VlpLNCRRFtZarRpn2lYYmsvhz0QtQTFyf/DNBaObjJMvxwOzcVFu1327lZZReNN62y0tvrvJqRhrK+5LN3XQruBAH6L220fQ9xODG2UAaKgNnK6LqsXm4XGepupaE2ElbTWlqtSW07XzB2LSat9Ncgi5TPunjU5Ds+y8f7U6eirIMuAVwa+qfjU4y4hswiZsiH03FkJrcyU4a3q5TaJoe22d/0i17++cmJAbw6S01BSttcc5VtBKi9AFv/B2jTIj4Bf/vkCDGHwzV9J40kvAaEfPiCI0y2UaeI/mlhncNpQ9t0egEsZ4j1ZgDP95lFgpR01avqXWWnVdqqa5SpCqU4nDYE9GyO/JmLo39tNAl6l8/w493CXBDd9zdzY26vsEs5bb6XbfoPUJZ9ZIV6MECTlFvWbvx3+h+fl38LJc3yXM//AZ+/R5/4fFLRAAAAAElFTkSuQmCC'); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"image-20210818200138093\"\n        title=\"image-20210818200138093\"\n        src=\"/static/503ecae512da477ed612539388f141bc/699b7/image-20210818200138093.png\"\n        srcset=\"/static/503ecae512da477ed612539388f141bc/5a46d/image-20210818200138093.png 300w,\n/static/503ecae512da477ed612539388f141bc/699b7/image-20210818200138093.png 596w\"\n        sizes=\"(max-width: 596px) 100vw, 596px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n      />\n    </span></p>\n<pre class=\"grvsc-container one-dark-pro\" data-language=\"\" data-index=\"0\"><code class=\"grvsc-code\"><span class=\"grvsc-line\"><span class=\"grvsc-source\">https://pytorch.org/tutorials/beginner/examples_nn/two_layer_net_optim.html</span></span>\n<span class=\"grvsc-line\"><span class=\"grvsc-source\"></span></span>\n<span class=\"grvsc-line\"><span class=\"grvsc-source\"># Before the backward pass, use the optimizer object to zero all of the</span></span>\n<span class=\"grvsc-line\"><span class=\"grvsc-source\">  # gradients for the variables it will update (which are the learnable</span></span>\n<span class=\"grvsc-line\"><span class=\"grvsc-source\">  # weights of the model). This is because by default, gradients are</span></span>\n<span class=\"grvsc-line\"><span class=\"grvsc-source\">  # accumulated in buffers( i.e, not overwritten) whenever .backward()</span></span>\n<span class=\"grvsc-line\"><span class=\"grvsc-source\">  # is called. Checkout docs of torch.autograd.backward for more details.</span></span>\n<span class=\"grvsc-line\"><span class=\"grvsc-source\">  optimizer.zero_grad()</span></span></code></pre>\n<h2 id=\"questions-during-nlp-study\" style=\"position:relative;\"><a href=\"#questions-during-nlp-study\" aria-label=\"questions during nlp study permalink\" class=\"toc-header before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Questions during NLP study</h2>\n<ul>\n<li>\n<h2 id=\"lstm-관련해서-토의-강의-14분-30초-부분이-특히-어려웠다고\" style=\"position:relative;\"><a href=\"#lstm-%EA%B4%80%EB%A0%A8%ED%95%B4%EC%84%9C-%ED%86%A0%EC%9D%98-%EA%B0%95%EC%9D%98-14%EB%B6%84-30%EC%B4%88-%EB%B6%80%EB%B6%84%EC%9D%B4-%ED%8A%B9%ED%9E%88-%EC%96%B4%EB%A0%A4%EC%9B%A0%EB%8B%A4%EA%B3%A0\" aria-label=\"lstm 관련해서 토의 강의 14분 30초 부분이 특히 어려웠다고 permalink\" class=\"toc-header before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>LSTM 관련해서 토의... 강의 14분 30초 부분이 특히 어려웠다고.</h2>\n</li>\n<li>i, f, o, g 간의 차이가 어떤 건지 모르겠다.</li>\n</ul>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 192px; \"\n    >\n      <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 16.666666666666664%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAADCAYAAACTWi8uAAAACXBIWXMAAAsSAAALEgHS3X78AAAAuklEQVQI1zWOwU7DMBBE8//fROkRoqiIE6WNHQopaSW7OHZSVzhxHq4lRlrt7GilecXgLG4YuSuEKW/zY5immL1Simme+dec/H1CCCzLkrP28MH7XmRflGWZDomzPQ+rRy5as16tMXbg3HU8PZe03yfGweHclbre87J5Zbfd8nk840dLVVW87WQuKPreIGVDmGMKIjfvEbWgt4k6PSitaJoD5qL5ak/8JjLvb+hE3h67TGUTjBCSOUb+AMd24eoBFpFnAAAAAElFTkSuQmCC'); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"img\"\n        title=\"img\"\n        src=\"/static/210b4bb06ef3375603c3261a599d72fd/8514f/https%253A%252F%252Fs3-us-west-2.amazonaws.com%252Fsecure.notion-static.com%252Fa4923480-caef-4508-9a69-df0da3031220%252FUntitled.png\"\n        srcset=\"/static/210b4bb06ef3375603c3261a599d72fd/8514f/https%253A%252F%252Fs3-us-west-2.amazonaws.com%252Fsecure.notion-static.com%252Fa4923480-caef-4508-9a69-df0da3031220%252FUntitled.png 192w\"\n        sizes=\"(max-width: 192px) 100vw, 192px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n      />\n    </span></p>\n<ul>\n<li>정보를 전달할 떄 tanh 를 취하는 이유는 무엇일까요?</li>\n<li>틸드 C_t를 왜 제한하는 걸까요? 단순히 다음 층에 필요 없는 정보는 빼고 전달해야 하기 때문인가요?</li>\n<li>GRU에서 rt에 대한 자세한 설명은 skip된 것 같습니다.</li>\n<li>h와 c의 차이는 무엇일까?: 기억 셀(c)은 LSTM끼리만 주고 받는 정보, 은닉 상태(h)는 LSTM 바깥으로 출력 되는 output으로 일단 이해하면 될 듯.</li>\n<li><a href=\"https://coding-yoon.tistory.com/132\">RNN에서 activiation function을 tanh(x)를 쓰는 이유는 뭘까?</a>\n→ 원래 있던 값을 그대로 가져가게끔 만들어져서 sigmoid를 쓰는 것보다 hyperbolic tangent를 쓰는 게 낫다고 판단했다. → BPTT를 쓰기 때문에 hyperbolic tangent는 미분값 range가 (0, 1) 사이이고, sigmoid는 (0, 0.25) 사이이기 때문에 이를 계속 곱해주면 sigmoid는 vanishing gradient 문제가 생긴다고 알고 있고, hyperbolic tangent는 그렇지 않다고 알고 있다. → 다만 ReLU를 BPTT 구조에서 사용하면 왜 안 되는지는 모르겠다. 누군가가 <code>RNN에서 relu를 사용하면 vanishing gradient 문제는 해결하지만 히든레이어 값이 exploding 하기 때문에 사용하지 않는다고 들었습니다.</code> 라고 얘기하기는 했다.</li>\n</ul>\n</li>\n</ul>\n<style class=\"grvsc-styles\">\n  .grvsc-container {\n    overflow: auto;\n    position: relative;\n    -webkit-overflow-scrolling: touch;\n    padding-top: 1rem;\n    padding-top: var(--grvsc-padding-top, var(--grvsc-padding-v, 1rem));\n    padding-bottom: 1rem;\n    padding-bottom: var(--grvsc-padding-bottom, var(--grvsc-padding-v, 1rem));\n    border-radius: 8px;\n    border-radius: var(--grvsc-border-radius, 8px);\n    font-feature-settings: normal;\n    line-height: 1.4;\n  }\n  \n  .grvsc-code {\n    display: table;\n  }\n  \n  .grvsc-line {\n    display: table-row;\n    box-sizing: border-box;\n    width: 100%;\n    position: relative;\n  }\n  \n  .grvsc-line > * {\n    position: relative;\n  }\n  \n  .grvsc-gutter-pad {\n    display: table-cell;\n    padding-left: 0.75rem;\n    padding-left: calc(var(--grvsc-padding-left, var(--grvsc-padding-h, 1.5rem)) / 2);\n  }\n  \n  .grvsc-gutter {\n    display: table-cell;\n    -webkit-user-select: none;\n    -moz-user-select: none;\n    user-select: none;\n  }\n  \n  .grvsc-gutter::before {\n    content: attr(data-content);\n  }\n  \n  .grvsc-source {\n    display: table-cell;\n    padding-left: 1.5rem;\n    padding-left: var(--grvsc-padding-left, var(--grvsc-padding-h, 1.5rem));\n    padding-right: 1.5rem;\n    padding-right: var(--grvsc-padding-right, var(--grvsc-padding-h, 1.5rem));\n  }\n  \n  .grvsc-source:empty::after {\n    content: ' ';\n    -webkit-user-select: none;\n    -moz-user-select: none;\n    user-select: none;\n  }\n  \n  .grvsc-gutter + .grvsc-source {\n    padding-left: 0.75rem;\n    padding-left: calc(var(--grvsc-padding-left, var(--grvsc-padding-h, 1.5rem)) / 2);\n  }\n  \n  /* Line transformer styles */\n  \n  .grvsc-has-line-highlighting > .grvsc-code > .grvsc-line::before {\n    content: ' ';\n    position: absolute;\n    width: 100%;\n  }\n  \n  .grvsc-line-diff-add::before {\n    background-color: var(--grvsc-line-diff-add-background-color, rgba(0, 255, 60, 0.2));\n  }\n  \n  .grvsc-line-diff-del::before {\n    background-color: var(--grvsc-line-diff-del-background-color, rgba(255, 0, 20, 0.2));\n  }\n  \n  .grvsc-line-number {\n    padding: 0 2px;\n    text-align: right;\n    opacity: 0.7;\n  }\n  \n  .one-dark-pro {\n    background-color: #282c34;\n    color: #abb2bf;\n  }\n  .one-dark-pro .grvsc-line-highlighted::before {\n    background-color: var(--grvsc-line-highlighted-background-color, rgba(255, 255, 255, 0.1));\n    box-shadow: inset var(--grvsc-line-highlighted-border-width, 4px) 0 0 0 var(--grvsc-line-highlighted-border-color, rgba(255, 255, 255, 0.5));\n  }\n</style>","tableOfContents":"<ul>\n<li><a href=\"/DL&#x26;ML/List-of-My-Questions/#questions-during-pytorch-study\">Questions during Pytorch Study</a></li>\n</ul>","frontmatter":{"date":"2021-08-17","title":"List of Unnoticed Questions","tags":["Pytorch","Theory"]}}},"pageContext":{"slug":"/DL&ML/List-of-My-Questions/","previous":{"fields":{"slug":"/Others/My-Studying-Routine/"},"frontmatter":{"title":"My Studying Routine"}},"next":{"fields":{"slug":"/DL&ML/Mask-Age-Gender-Classification-Competition/"},"frontmatter":{"title":"Classification Competition Wrap up report 1"}}}},"staticQueryHashes":["1081905842","3911196313"]}