{"componentChunkName":"component---src-pages-about-js","path":"/about/","result":{"data":{"allMarkdownRemark":{"edges":[{"node":{"excerpt":"","html":"<style class=\"grvsc-styles\">\n  .grvsc-container {\n    overflow: auto;\n    position: relative;\n    -webkit-overflow-scrolling: touch;\n    padding-top: 1rem;\n    padding-top: var(--grvsc-padding-top, var(--grvsc-padding-v, 1rem));\n    padding-bottom: 1rem;\n    padding-bottom: var(--grvsc-padding-bottom, var(--grvsc-padding-v, 1rem));\n    border-radius: 8px;\n    border-radius: var(--grvsc-border-radius, 8px);\n    font-feature-settings: normal;\n    line-height: 1.4;\n  }\n  \n  .grvsc-code {\n    display: table;\n  }\n  \n  .grvsc-line {\n    display: table-row;\n    box-sizing: border-box;\n    width: 100%;\n    position: relative;\n  }\n  \n  .grvsc-line > * {\n    position: relative;\n  }\n  \n  .grvsc-gutter-pad {\n    display: table-cell;\n    padding-left: 0.75rem;\n    padding-left: calc(var(--grvsc-padding-left, var(--grvsc-padding-h, 1.5rem)) / 2);\n  }\n  \n  .grvsc-gutter {\n    display: table-cell;\n    -webkit-user-select: none;\n    -moz-user-select: none;\n    user-select: none;\n  }\n  \n  .grvsc-gutter::before {\n    content: attr(data-content);\n  }\n  \n  .grvsc-source {\n    display: table-cell;\n    padding-left: 1.5rem;\n    padding-left: var(--grvsc-padding-left, var(--grvsc-padding-h, 1.5rem));\n    padding-right: 1.5rem;\n    padding-right: var(--grvsc-padding-right, var(--grvsc-padding-h, 1.5rem));\n  }\n  \n  .grvsc-source:empty::after {\n    content: ' ';\n    -webkit-user-select: none;\n    -moz-user-select: none;\n    user-select: none;\n  }\n  \n  .grvsc-gutter + .grvsc-source {\n    padding-left: 0.75rem;\n    padding-left: calc(var(--grvsc-padding-left, var(--grvsc-padding-h, 1.5rem)) / 2);\n  }\n  \n  /* Line transformer styles */\n  \n  .grvsc-has-line-highlighting > .grvsc-code > .grvsc-line::before {\n    content: ' ';\n    position: absolute;\n    width: 100%;\n  }\n  \n  .grvsc-line-diff-add::before {\n    background-color: var(--grvsc-line-diff-add-background-color, rgba(0, 255, 60, 0.2));\n  }\n  \n  .grvsc-line-diff-del::before {\n    background-color: var(--grvsc-line-diff-del-background-color, rgba(255, 0, 20, 0.2));\n  }\n  \n  .grvsc-line-number {\n    padding: 0 2px;\n    text-align: right;\n    opacity: 0.7;\n  }\n  \n</style>","fields":{"slug":"/Researching/Dataset-Papers/"},"frontmatter":{"date":"2022-10-01","title":"Well-written Dataset Papers","draft":true}}},{"node":{"excerpt":"https://github.com/QuoQA-NLP/MRC_Baseline https://github.com/QuoQA-NLP/QAbaseline","html":"<p><a href=\"https://github.com/QuoQA-NLP/MRC_Baseline\">https://github.com/QuoQA-NLP/MRC_Baseline</a></p>\n<p><a href=\"https://github.com/QuoQA-NLP/QAbaseline\">https://github.com/QuoQA-NLP/QAbaseline</a></p>\n<style class=\"grvsc-styles\">\n  .grvsc-container {\n    overflow: auto;\n    position: relative;\n    -webkit-overflow-scrolling: touch;\n    padding-top: 1rem;\n    padding-top: var(--grvsc-padding-top, var(--grvsc-padding-v, 1rem));\n    padding-bottom: 1rem;\n    padding-bottom: var(--grvsc-padding-bottom, var(--grvsc-padding-v, 1rem));\n    border-radius: 8px;\n    border-radius: var(--grvsc-border-radius, 8px);\n    font-feature-settings: normal;\n    line-height: 1.4;\n  }\n  \n  .grvsc-code {\n    display: table;\n  }\n  \n  .grvsc-line {\n    display: table-row;\n    box-sizing: border-box;\n    width: 100%;\n    position: relative;\n  }\n  \n  .grvsc-line > * {\n    position: relative;\n  }\n  \n  .grvsc-gutter-pad {\n    display: table-cell;\n    padding-left: 0.75rem;\n    padding-left: calc(var(--grvsc-padding-left, var(--grvsc-padding-h, 1.5rem)) / 2);\n  }\n  \n  .grvsc-gutter {\n    display: table-cell;\n    -webkit-user-select: none;\n    -moz-user-select: none;\n    user-select: none;\n  }\n  \n  .grvsc-gutter::before {\n    content: attr(data-content);\n  }\n  \n  .grvsc-source {\n    display: table-cell;\n    padding-left: 1.5rem;\n    padding-left: var(--grvsc-padding-left, var(--grvsc-padding-h, 1.5rem));\n    padding-right: 1.5rem;\n    padding-right: var(--grvsc-padding-right, var(--grvsc-padding-h, 1.5rem));\n  }\n  \n  .grvsc-source:empty::after {\n    content: ' ';\n    -webkit-user-select: none;\n    -moz-user-select: none;\n    user-select: none;\n  }\n  \n  .grvsc-gutter + .grvsc-source {\n    padding-left: 0.75rem;\n    padding-left: calc(var(--grvsc-padding-left, var(--grvsc-padding-h, 1.5rem)) / 2);\n  }\n  \n  /* Line transformer styles */\n  \n  .grvsc-has-line-highlighting > .grvsc-code > .grvsc-line::before {\n    content: ' ';\n    position: absolute;\n    width: 100%;\n  }\n  \n  .grvsc-line-diff-add::before {\n    background-color: var(--grvsc-line-diff-add-background-color, rgba(0, 255, 60, 0.2));\n  }\n  \n  .grvsc-line-diff-del::before {\n    background-color: var(--grvsc-line-diff-del-background-color, rgba(255, 0, 20, 0.2));\n  }\n  \n  .grvsc-line-number {\n    padding: 0 2px;\n    text-align: right;\n    opacity: 0.7;\n  }\n  \n</style>","fields":{"slug":"/Engineering/NIPA-MRC-Competition/"},"frontmatter":{"date":"2022-10-01","title":"Recap for National IT Industry Promotion Agency's Machine Reading Comprehension Competetion","draft":true}}},{"node":{"excerpt":"","html":"<style class=\"grvsc-styles\">\n  .grvsc-container {\n    overflow: auto;\n    position: relative;\n    -webkit-overflow-scrolling: touch;\n    padding-top: 1rem;\n    padding-top: var(--grvsc-padding-top, var(--grvsc-padding-v, 1rem));\n    padding-bottom: 1rem;\n    padding-bottom: var(--grvsc-padding-bottom, var(--grvsc-padding-v, 1rem));\n    border-radius: 8px;\n    border-radius: var(--grvsc-border-radius, 8px);\n    font-feature-settings: normal;\n    line-height: 1.4;\n  }\n  \n  .grvsc-code {\n    display: table;\n  }\n  \n  .grvsc-line {\n    display: table-row;\n    box-sizing: border-box;\n    width: 100%;\n    position: relative;\n  }\n  \n  .grvsc-line > * {\n    position: relative;\n  }\n  \n  .grvsc-gutter-pad {\n    display: table-cell;\n    padding-left: 0.75rem;\n    padding-left: calc(var(--grvsc-padding-left, var(--grvsc-padding-h, 1.5rem)) / 2);\n  }\n  \n  .grvsc-gutter {\n    display: table-cell;\n    -webkit-user-select: none;\n    -moz-user-select: none;\n    user-select: none;\n  }\n  \n  .grvsc-gutter::before {\n    content: attr(data-content);\n  }\n  \n  .grvsc-source {\n    display: table-cell;\n    padding-left: 1.5rem;\n    padding-left: var(--grvsc-padding-left, var(--grvsc-padding-h, 1.5rem));\n    padding-right: 1.5rem;\n    padding-right: var(--grvsc-padding-right, var(--grvsc-padding-h, 1.5rem));\n  }\n  \n  .grvsc-source:empty::after {\n    content: ' ';\n    -webkit-user-select: none;\n    -moz-user-select: none;\n    user-select: none;\n  }\n  \n  .grvsc-gutter + .grvsc-source {\n    padding-left: 0.75rem;\n    padding-left: calc(var(--grvsc-padding-left, var(--grvsc-padding-h, 1.5rem)) / 2);\n  }\n  \n  /* Line transformer styles */\n  \n  .grvsc-has-line-highlighting > .grvsc-code > .grvsc-line::before {\n    content: ' ';\n    position: absolute;\n    width: 100%;\n  }\n  \n  .grvsc-line-diff-add::before {\n    background-color: var(--grvsc-line-diff-add-background-color, rgba(0, 255, 60, 0.2));\n  }\n  \n  .grvsc-line-diff-del::before {\n    background-color: var(--grvsc-line-diff-del-background-color, rgba(255, 0, 20, 0.2));\n  }\n  \n  .grvsc-line-number {\n    padding: 0 2px;\n    text-align: right;\n    opacity: 0.7;\n  }\n  \n</style>","fields":{"slug":"/Configuring/Pleading-for-Github-Blogs/"},"frontmatter":{"date":"2022-10-01","title":"Choosing github.io blog over other platforms","draft":true}}},{"node":{"excerpt":"üìñ Education Yonsei University, Seoul Campus Student Clubs GDSC Yonsei University Seoul Campus, ML Core Computer Science Club PoolC, Academics Manager üèÜ Competition Awards Topic / Task Result Machine ‚Ä¶","html":"<h2 id=\"-education\" style=\"position:relative;\"><a href=\"#-education\" aria-label=\" education permalink\" class=\"toc-header before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>üìñ Education</h2>\n<h3 id=\"yonsei-university-seoul-campus\" style=\"position:relative;\"><a href=\"#yonsei-university-seoul-campus\" aria-label=\"yonsei university seoul campus permalink\" class=\"toc-header before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Yonsei University, Seoul Campus</h3>\n<details>\n<summary> Bachelor of Economics (Major) & Applied Statistics (Minor)</summary>\n  <ul>\n    <li>INTRODUCTION TO STATISTICS (A0)</li>\n    <li>STATISTICAL METHOD (A+)</li>\n    <li>CALCULUS (TBD)</li>\n    <li>LINEAR ALGEBRA (B+)</li>\n    <li>MATHEMATICAL STATISTICS 1 (A+)</li>\n    <li>LINEAR REGRESSION (B+)</li>\n    <li>R AND PYTHON PROGRAMMING (A+)</li>\n    <li>DATA STRUCTURE (TBD)</li>\n    <li>SPECIAL PROBLEMS IN COMPUTING (A0)</li>\n    <li>SOCIAL INFORMATICS (A+)</li>\n    <li>TIME SERIES ANALYSIS (A+)</li>\n  </ul>\n</details>\n<p><strong>Student Clubs</strong></p>\n<ul>\n<li><a href=\"https://github.com/gdsc-ys/\">GDSC Yonsei University Seoul Campus, ML Core</a></li>\n<li><a href=\"https://github.com/poolc\">Computer Science Club PoolC, Academics Manager</a></li>\n</ul>\n<hr>\n<h2 id=\"-competition-awards\" style=\"position:relative;\"><a href=\"#-competition-awards\" aria-label=\" competition awards permalink\" class=\"toc-header before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>üèÜ Competition Awards</h2>\n<table>\n<thead>\n<tr>\n<th align=\"center\">Topic / Task</th>\n<th align=\"center\">Result</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td align=\"center\"><a href=\"https://github.com/QuoQA-NLP/MRC_Baseline\">Machine Reading <br />Compehension</a></td>\n<td align=\"center\">ü•à 2nd <br />(2/26)</td>\n</tr>\n<tr>\n<td align=\"center\"><a href=\"https://data.kostat.go.kr/sbchome/bbs/boardList.do?boardId=SBCSBBS_000000025000&#x26;curMenuNo=OPT_09_02_00_0\">Korean Standard <br />Industry Classification</a></td>\n<td align=\"center\">üéñ 7th <br />(7/311)</td>\n</tr>\n<tr>\n<td align=\"center\"><a href=\"https://dacon.io/competitions/official/235875/codeshare/4589\">KLUE benchmark <br />Natural Language Inference</a></td>\n<td align=\"center\">ü•á 1st <br />(1/468)</td>\n</tr>\n<tr>\n<td align=\"center\"><a href=\"https://github.com/sangHa0411/CloneDetection\">Python Code<br />Clone Detection</a></td>\n<td align=\"center\">ü•â 3rd <br />(3/337)</td>\n</tr>\n<tr>\n<td align=\"center\"><a href=\"https://github.com/snoop2head/elastic-stock-prediction\">Stock Price Forecast<br />on KOSPI &#x26; KOSDAQ</a></td>\n<td align=\"center\">üéñ 6th <br />(6/205)</td>\n</tr>\n</tbody>\n</table>\n<p>**Dacon is Kaggle alike competition platform in Korea.</p>\n<hr>\n<h2 id=\"-multimodal-projects\" style=\"position:relative;\"><a href=\"#-multimodal-projects\" aria-label=\" multimodal projects permalink\" class=\"toc-header before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>üõ† Multimodal Projects</h2>\n<h3 id=\"kodalle-text-to-fashion-2021\" style=\"position:relative;\"><a href=\"#kodalle-text-to-fashion-2021\" aria-label=\"kodalle text to fashion 2021 permalink\" class=\"toc-header before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a><a href=\"https://github.com/KR-HappyFace/KoDALLE\">KoDALLE: Text to Fashion (2021)</a></h3>\n<p><a href=\"https://github.com/KR-HappyFace/KoDALLE\"><img width=\"700\" alt=\"image\" src=\"https://github.com/KR-HappyFace/KoDALLE/raw/main/assets/README/image-20211227151557604.png\"></a></p>\n<p><strong>Generating dress outfit images based on given input text</strong> | <a href=\"https://github.com/KR-HappyFace/KoDALLE/blob/main/README.pdf\">üìÑ Presentation</a></p>\n<ul>\n<li><strong>Created training pipeline from VQGAN through DALLE</strong></li>\n<li><strong>Maintained versions of 1 million pairs image-caption dataset.</strong></li>\n<li>Trained VQGAN and DALLE model from the scratch.</li>\n<li>Established live demo for the KoDALLE on Huggingface Space via FastAPI.</li>\n</ul>\n<h2 id=\"-deep-learning-security-projects\" style=\"position:relative;\"><a href=\"#-deep-learning-security-projects\" aria-label=\" deep learning security projects permalink\" class=\"toc-header before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>üîê Deep Learning Security Projects</h2>\n<h3 id=\"language-model-memorization-2022\" style=\"position:relative;\"><a href=\"#language-model-memorization-2022\" aria-label=\"language model memorization 2022 permalink\" class=\"toc-header before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a><a href=\"https://github.com/yonsei-cysec/Language_Model_Memorization\">Language Model Memorization (2022)</a></h3>\n<p><strong>Implementation of Carlini et al(2020) <a href=\"https://arxiv.org/abs/2012.07805\">Extracting Training Data from Large Language Models</a></strong></p>\n<ul>\n<li>Accelerated inference speed with parallel Multi-GPU usage.</li>\n<li>Ruled out 'low-quality repeated generations' problem of the paper with repetition penalty and with ngram restriction.</li>\n</ul>\n<h3 id=\"membership-inference-attack-2022\" style=\"position:relative;\"><a href=\"#membership-inference-attack-2022\" aria-label=\"membership inference attack 2022 permalink\" class=\"toc-header before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a><a href=\"https://github.com/yonsei-cysec/Membership_Inference_Attack\">Membership Inference Attack (2022)</a></h3>\n<p><strong>Implementation of Shokri et al(2016) <a href=\"https://arxiv.org/pdf/1610.05820.pdf\">Membership Inference Attacks Against Machine Learning Models</a></strong></p>\n<ul>\n<li>Prevented overfitting of shadow models' by adding early stop, regularizing with weight decay and allocating train/val/test datasets.</li>\n<li>Referenced <a href=\"https://arxiv.org/abs/2112.03570\">Carlini et al(2021)</a> to conduct further research on different types of models and metrics.</li>\n<li>Reproduced attack metrics as the following.</li>\n</ul>\n<table>\n<thead>\n<tr>\n<th align=\"center\">MIA Attack Metrics</th>\n<th align=\"center\">Accuracy</th>\n<th align=\"center\">Precision</th>\n<th align=\"center\">Recall</th>\n<th align=\"center\">F1 Score</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td align=\"center\">CIFAR10</td>\n<td align=\"center\">0.7761</td>\n<td align=\"center\">0.7593</td>\n<td align=\"center\">0.8071</td>\n<td align=\"center\">0.7825</td>\n</tr>\n<tr>\n<td align=\"center\">CIFAR100</td>\n<td align=\"center\">0.9746</td>\n<td align=\"center\">0.9627</td>\n<td align=\"center\">0.9875</td>\n<td align=\"center\">0.9749</td>\n</tr>\n</tbody>\n</table>\n<table>\n<thead>\n<tr>\n<th align=\"center\">MIA ROC Curve CIFAR10</th>\n<th align=\"center\">MIA ROC Curve CIFAR100</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td align=\"center\"><img src=\"https://raw.githubusercontent.com/snoop2head/Membership_Inference_Attack/main/assets/roc_cifar10.png\" alt=\"roc_curve CIFAR10\"></td>\n<td align=\"center\"><img src=\"https://raw.githubusercontent.com/snoop2head/Membership_Inference_Attack/main/assets/roc_cifar100.png\" alt=\"roc_curve CIFAR100\"></td>\n</tr>\n</tbody>\n</table>\n<h2 id=\"-natural-language-processing-projects\" style=\"position:relative;\"><a href=\"#-natural-language-processing-projects\" aria-label=\" natural language processing projects permalink\" class=\"toc-header before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>üí¨ Natural Language Processing Projects</h2>\n<h3 id=\"koquillbot-2022--t5-translation-2022\" style=\"position:relative;\"><a href=\"#koquillbot-2022--t5-translation-2022\" aria-label=\"koquillbot 2022  t5 translation 2022 permalink\" class=\"toc-header before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a><a href=\"https://github.com/QuoQA-NLP/KoQuillBot\">KoQuillBot (2022)</a> &#x26; <a href=\"https://github.com/QuoQA-NLP/T5_Translation\">T5 Translation (2022)</a></h3>\n<p><strong>Paraphrasing tool with round trip translation utilizing T5 Machine Translation.</strong> | <a href=\"https://huggingface.co/spaces/QuoQA-NLP/KoQuillBot\">ü§ó KoQuillBot Demo</a> &#x26; <a href=\"https://huggingface.co/spaces/QuoQA-NLP/QuoQaGo\">ü§ó Translator Demo</a></p>\n<table>\n<thead>\n<tr>\n<th align=\"center\"></th>\n<th align=\"center\">BLEU Score</th>\n<th align=\"center\">Translation Result</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td align=\"center\">Korean ‚û°Ô∏è English</td>\n<td align=\"center\">45.15</td>\n<td align=\"center\"><a href=\"https://huggingface.co/datasets/QuoQA-NLP/KE-T5-Ko2En-Base-Inference-Result\">üîó Inference Result</a></td>\n</tr>\n<tr>\n<td align=\"center\">English ‚û°Ô∏è Korean</td>\n<td align=\"center\">-</td>\n<td align=\"center\">-</td>\n</tr>\n</tbody>\n</table>\n<h3 id=\"deep-encoder-shallow-decoder-2022\" style=\"position:relative;\"><a href=\"#deep-encoder-shallow-decoder-2022\" aria-label=\"deep encoder shallow decoder 2022 permalink\" class=\"toc-header before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a><a href=\"https://github.com/snoop2head/Deep-Encoder-Shallow-Decoder\">Deep Encoder Shallow Decoder (2022)</a></h3>\n<p><strong>Implementation of Kasai et al(2020) <a href=\"https://arxiv.org/abs/2006.10369\">Deep Encoder, Shallow Decoder: Reevaluating Non-autoregressive Machine Translation</a></strong> | <a href=\"https://docs.google.com/spreadsheets/d/1IqEuRuEpphPEX3ni1m0EwqYuOU4E4t4-jC6uullpJhE/edit#gid=204599913\">üìÑ Translation Output</a></p>\n<ul>\n<li>Composed custom dataset, trainer, inference code in pytorch and huggingface.</li>\n<li>Trained and hosted encoder-decoder transformers model using huggingface.</li>\n</ul>\n<table>\n<thead>\n<tr>\n<th align=\"center\"></th>\n<th align=\"center\">BLEU Score</th>\n<th align=\"center\">Translation Result</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td align=\"center\">Korean ‚û°Ô∏è English</td>\n<td align=\"center\">35.82</td>\n<td align=\"center\"><a href=\"https://docs.google.com/spreadsheets/d/1IqEuRuEpphPEX3ni1m0EwqYuOU4E4t4-jC6uullpJhE/edit#gid=204599913\">üîó Inference Result</a></td>\n</tr>\n<tr>\n<td align=\"center\">English ‚û°Ô∏è Korean</td>\n<td align=\"center\">-</td>\n<td align=\"center\">-</td>\n</tr>\n</tbody>\n</table>\n<h3 id=\"klue-rbert-2021\" style=\"position:relative;\"><a href=\"#klue-rbert-2021\" aria-label=\"klue rbert 2021 permalink\" class=\"toc-header before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a><a href=\"https://github.com/snoop2head/KLUE-RBERT\">KLUE-RBERT (2021)</a></h3>\n<p><strong>Extracting relations between subject and object entity in KLUE Benchmark dataset</strong> | <a href=\"https://snoop2head.github.io/Engineering/Relation-Extraction-Code/\">‚úçÔ∏è Blog Post</a></p>\n<ul>\n<li>Finetuned RoBERTa model according to RBERT structure in pytorch.</li>\n<li>Applied stratified k-fold cross validation for the custom trainer.</li>\n</ul>\n<h3 id=\"conditional-generation-with-kogpt-2021\" style=\"position:relative;\"><a href=\"#conditional-generation-with-kogpt-2021\" aria-label=\"conditional generation with kogpt 2021 permalink\" class=\"toc-header before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a><a href=\"https://github.com/snoop2head/KoGPT-Joong-2\">Conditional Generation with KoGPT (2021)</a></h3>\n<p><strong>Sentence generation with given emotion conditions</strong> | <a href=\"https://huggingface.co/spaces/snoop2head/KoGPT-Conditional-Generation\">ü§ó Huggingface Demo</a></p>\n<ul>\n<li>Finetuned KoGPT-Trinity with conditional emotion labels.</li>\n<li>Maintained huggingface hosted model and live demo.</li>\n</ul>\n<h3 id=\"machine-reading-comprehension-in-naver-boostcamp-2021\" style=\"position:relative;\"><a href=\"#machine-reading-comprehension-in-naver-boostcamp-2021\" aria-label=\"machine reading comprehension in naver boostcamp 2021 permalink\" class=\"toc-header before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a><a href=\"https://snoop2head.github.io/Custom-MRC-Reader/\">Machine Reading Comprehension in Naver Boostcamp (2021)</a></h3>\n<p><strong>Retrieved and extracted answers from wikipedia texts for given question</strong> | <a href=\"https://snoop2head.github.io/Engineering/Custom-MRC-Reader/\">‚úçÔ∏è Blog Post</a></p>\n<ul>\n<li>Attached bidirectional LSTM layers to the backbone transformers model to extract answers.</li>\n<li>Divided benchmark into start token prediction accuracy and end token prediction accuracy.</li>\n</ul>\n<h3 id=\"mathpresso-corporation-joint-project-2020\" style=\"position:relative;\"><a href=\"#mathpresso-corporation-joint-project-2020\" aria-label=\"mathpresso corporation joint project 2020 permalink\" class=\"toc-header before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a><a href=\"https://github.com/snoop2head/Mathpresso_Classification\">Mathpresso Corporation Joint Project (2020)</a></h3>\n<p><strong>Corporate joint project for mathematics problems classification task</strong> | <a href=\"https://github.com/snoop2head/Mathpresso_Classification/blob/main/YBIGTA_%EB%A7%A4%EC%93%B0%ED%94%84%EB%A0%88%EC%86%8C_%EB%AA%BD%EB%8D%B0%EC%9D%B4%ED%81%AC_Final.pdf\">üìÑ Presentation</a></p>\n<ul>\n<li>Preprocessed Korean mathematics problems dataset based on EDA.</li>\n<li>Maintained version of preprocessing module.</li>\n</ul>\n<h3 id=\"constructing-emotional-instagram-posts-dataset-2019\" style=\"position:relative;\"><a href=\"#constructing-emotional-instagram-posts-dataset-2019\" aria-label=\"constructing emotional instagram posts dataset 2019 permalink\" class=\"toc-header before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a><a href=\"https://github.com/Keracorn/geulstagram\">Constructing Emotional Instagram Posts Dataset (2019)</a></h3>\n<p><strong>Created Emotional Instagram Posts(Í∏ÄÏä§ÌÉÄÍ∑∏Îû®) dataset</strong> | <a href=\"https://github.com/Keracorn/geulstagram/blob/master/README.pdf\">üìÑ Presentation</a></p>\n<ul>\n<li>Managed version control for the project Github Repository.</li>\n<li>Converted Korean texts on image file into text file using Google Cloud Vision API.</li>\n</ul>\n<h2 id=\"-computer-vision-projects\" style=\"position:relative;\"><a href=\"#-computer-vision-projects\" aria-label=\" computer vision projects permalink\" class=\"toc-header before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>üëÄ Computer Vision Projects</h2>\n<h3 id=\"elimnet-2021\" style=\"position:relative;\"><a href=\"#elimnet-2021\" aria-label=\"elimnet 2021 permalink\" class=\"toc-header before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a><a href=\"https://github.com/snoop2head/ELimNet\">ElimNet (2021)</a></h3>\n<p><strong>Elimination based Lightweight Neural Net with Pretrained Weights</strong> | <a href=\"https://github.com/snoop2head/ELimNet/blob/main/README.pdf\">üìÑ Presentation</a></p>\n<ul>\n<li>Constructed lightweight CNN model with less than 1M #params by removing top layers from pretrained CNN models.</li>\n<li>Assessed on Trash Annotations in Context(TACO) Dataset sampled for 6 classes with 20,851 images.</li>\n<li>Compared metrics accross VGG11, MobileNetV3 and EfficientNetB0.</li>\n</ul>\n<h3 id=\"face-mask-age-gender-classification-in-naver-boostcamp-2021\" style=\"position:relative;\"><a href=\"#face-mask-age-gender-classification-in-naver-boostcamp-2021\" aria-label=\"face mask age gender classification in naver boostcamp 2021 permalink\" class=\"toc-header before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a><a href=\"https://github.com/boostcampaitech2/image-classification-level1-23\">Face Mask, Age, Gender Classification in Naver Boostcamp (2021)</a></h3>\n<p><strong>Identifying 18 classes from given images: Age Range(3 classes), Biological Sex(2 classes), Face Mask(3 classes)</strong> | <a href=\"https://snoop2head.github.io/Engineering/Mask-Age-Gender-Classification-Competition/\">‚úçÔ∏è Blog Post</a></p>\n<ul>\n<li>Optimized combination of backbone models, losses and optimizers.</li>\n<li>Created additional dataset with labels(age, sex, mask) to resolve class imbalance.</li>\n<li>Cropped facial characteristics with MTCNN and RetinaFace to reduce noise in the image.</li>\n</ul>\n<h3 id=\"realtime-desktop-posture-classification-2020\" style=\"position:relative;\"><a href=\"#realtime-desktop-posture-classification-2020\" aria-label=\"realtime desktop posture classification 2020 permalink\" class=\"toc-header before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a><a href=\"https://github.com/snoop2head/ml_classification_tutorial\">Realtime Desktop Posture Classification (2020)</a></h3>\n<p><strong>Real-time desk posture classification through webcam</strong> | <a href=\"https://www.youtube.com/watch?v=6z_TJaj71io&#x26;t=459s\">üì∑ Demo Video</a></p>\n<ul>\n<li>Created real-time detection window using opencv-python.</li>\n<li>Converted image dataset into Yaw/Pitch/Roll numerical dataset using RetinaFace model.</li>\n<li>Trained and optimized random forest classification model with precision rate of 93%.</li>\n</ul>\n<h2 id=\"-web-projects\" style=\"position:relative;\"><a href=\"#-web-projects\" aria-label=\" web projects permalink\" class=\"toc-header before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>üï∏ Web Projects</h2>\n<h3 id=\"exchange-program-overview-website-2020\" style=\"position:relative;\"><a href=\"#exchange-program-overview-website-2020\" aria-label=\"exchange program overview website 2020 permalink\" class=\"toc-header before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a><a href=\"https://github.com/snoop2head/yonsei-exchange-program\">Exchange Program Overview Website (2020)</a></h3>\n<p><strong>Overview for student life in foreign universities</strong> | <a href=\"https://yonsei-exchange.netlify.app/\">‚úàÔ∏è Website Demo</a></p>\n<ul>\n<li><strong>3400 Visitors within a year (2021.07 ~ 2022.07)</strong></li>\n<li><strong>22000 Pageviews within a year (2021.07 ~ 2022.07)</strong></li>\n<li><strong>3 minutes+ of Average Retention Time</strong></li>\n<li>Collected and preprocessed 11200 text review data from the Yonsei website using pandas.</li>\n<li>Visualized department distribution and weather information using matplotlib.</li>\n<li>Sentiment analysis on satisfaction level for foreign universities with pretrained BERT model.</li>\n<li>Clustered universities with provided curriculum with K-means clustering.</li>\n<li>Hosted reports on universities using Gatsby.js, GraphQL, and Netlify.</li>\n</ul>\n<h3 id=\"fitcuration-website-2020\" style=\"position:relative;\"><a href=\"#fitcuration-website-2020\" aria-label=\"fitcuration website 2020 permalink\" class=\"toc-header before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a><a href=\"https://github.com/snoop2head/fitcuration-django\">fitcuration website (2020)</a></h3>\n<p><strong>Search-based exercise retrieval web service</strong> | <a href=\"https://youtu.be/kef0CxzMANo?t=38\">üì∑ Demo Video</a></p>\n<ul>\n<li>Built retrieval algorithm based on search keyword using TF-IDF.</li>\n<li>Deployed website using Docker, AWS RDS, AWS S3, AWS EBS</li>\n<li>Constructed backend using Django, Django ORM &#x26; PostgreSQL.</li>\n<li>Composed client-side using Sass, Tailwind, HTML5.</li>\n</ul>\n<h2 id=\"-quantitative-finance-projects\" style=\"position:relative;\"><a href=\"#-quantitative-finance-projects\" aria-label=\" quantitative finance projects permalink\" class=\"toc-header before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>üí∞ Quantitative Finance Projects</h2>\n<h3 id=\"forecasting-federal-rate-with-lasso-regression-model-2022\" style=\"position:relative;\"><a href=\"#forecasting-federal-rate-with-lasso-regression-model-2022\" aria-label=\"forecasting federal rate with lasso regression model 2022 permalink\" class=\"toc-header before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a><a href=\"https://github.com/snoop2head/Federal-Rate-Prediction\">Forecasting Federal Rate with Lasso Regression Model (2022)</a></h3>\n<p><strong>Federal Rate Prediction for the next FOMC Meeting</strong></p>\n<ul>\n<li>Wrangled quantitative dataset with Finance Data Reader.</li>\n<li>Yielded metrics and compared candidate regression models for the adaquate fit.</li>\n<li>Hyperparameter optimization for the candidate models.</li>\n</ul>\n<h3 id=\"korean-spinoff-event-tracker-2020\" style=\"position:relative;\"><a href=\"#korean-spinoff-event-tracker-2020\" aria-label=\"korean spinoff event tracker 2020 permalink\" class=\"toc-header before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a><a href=\"https://github.com/snoop2head/spinoff_hunter_kor\">Korean Spinoff Event Tracker (2020)</a></h3>\n<p><strong>Get financial data of public companies involved in spinoff events on Google Spreadsheet</strong> | <a href=\"https://docs.google.com/spreadsheets/d/1chJ2NKHVc0gKjsMaQI1UHEPxdjneV1ZWaTGHseQvxP4/edit?usp=sharing\">üß© Dataset Demo</a></p>\n<ul>\n<li>Wrangled finance dataset which are displayed on Google Sheets</li>\n</ul>\n<hr>\n<h2 id=\"-opensource-contributions\" style=\"position:relative;\"><a href=\"#-opensource-contributions\" aria-label=\" opensource contributions permalink\" class=\"toc-header before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>üè∑ Opensource Contributions</h2>\n<h3 id=\"nvlabsstylegan2-ada-pytorch-2021\" style=\"position:relative;\"><a href=\"#nvlabsstylegan2-ada-pytorch-2021\" aria-label=\"nvlabsstylegan2 ada pytorch 2021 permalink\" class=\"toc-header before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a><a href=\"https://github.com/NVlabs/stylegan2-ada-pytorch\">NVlabs/stylegan2-ada-pytorch (2021)</a></h3>\n<p><strong>Fixed torch version comparison fallback error for source repo of NVIDIA Research</strong> | <a href=\"https://github.com/NVlabs/stylegan2-ada-pytorch/pull/197\">‚úçÔ∏è Pull Request</a></p>\n<ul>\n<li>Skills: torch, torchvision</li>\n</ul>\n<h3 id=\"dockerdockergithubio-2020\" style=\"position:relative;\"><a href=\"#dockerdockergithubio-2020\" aria-label=\"dockerdockergithubio 2020 permalink\" class=\"toc-header before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a><a href=\"https://github.com/docker/docker.github.io\">docker/docker.github.io (2020)</a></h3>\n<p><strong>Updated PostgreSQL initialization for \"Quickstart: dockerizing django\" documentation</strong> | <a href=\"https://github.com/docker/docker.github.io/pull/10624\">üê≥ Pull Request</a></p>\n<ul>\n<li>Skills: Docker, docker-compose, Django</li>\n</ul>\n<h2 id=\"-etcs\" style=\"position:relative;\"><a href=\"#-etcs\" aria-label=\" etcs permalink\" class=\"toc-header before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>üóÑ ETCs</h2>\n<h3 id=\"covid19-confirmed-cases-prediction-2020\" style=\"position:relative;\"><a href=\"#covid19-confirmed-cases-prediction-2020\" aria-label=\"covid19 confirmed cases prediction 2020 permalink\" class=\"toc-header before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a><a href=\"https://github.com/Rank23/COVID19\">Covid19 Confirmed Cases Prediction (2020)</a></h3>\n<p><strong>Predict the spread of COVID-19 in early stage after its entrance to country.</strong></p>\n<ul>\n<li>Fixed existing errors on Github Repository.</li>\n<li>Wrote footnotes in both English and Korean.</li>\n<li>¬±5% accuracy for one-day prediction.</li>\n<li>¬±10% accuracy for 30-day prediction.</li>\n</ul>\n<h3 id=\"indigo-2019\" style=\"position:relative;\"><a href=\"#indigo-2019\" aria-label=\"indigo 2019 permalink\" class=\"toc-header before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a><a href=\"https://github.com/snoop2head/indigo\">Indigo (2019)</a></h3>\n<p><strong>Don't miss concerts for your favorite artists with KakaoTalk Chatbot</strong> | <a href=\"https://www.youtube.com/watch?v=uIOWqumaOD4\">üì∑ Demo Video</a></p>\n<ul>\n<li>Created API server for KakaoTalk chatbot with Flask, Pymongo and MongoDB.</li>\n<li>Deployed the API server on AWS EC2.</li>\n<li>Visualized concert schedules on user's Google Calendar.</li>\n<li>Created / Updated events in Google Calendar.</li>\n</ul>\n<hr>\n<h2 id=\"-skillsets\" style=\"position:relative;\"><a href=\"#-skillsets\" aria-label=\" skillsets permalink\" class=\"toc-header before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>üõ† Skillsets</h2>\n<p><strong>Data Analysis and Machine Learning</strong></p>\n<ul>\n<li>Data Analysis Library: pandas, numpy</li>\n<li>Deep Learning: pytorch, transformers</li>\n<li>Machine Learning: scikit-learn, gensim, xgboost</li>\n</ul>\n<p><strong>Backend</strong></p>\n<ul>\n<li>Python / Django - Django ORM, CRUD, OAuth</li>\n<li>Python / FastAPI(uvicorn) - CRUD API</li>\n<li>Python / Flask - CRUD API</li>\n</ul>\n<p><strong>Client</strong></p>\n<ul>\n<li>HTML / Pug.js</li>\n<li>CSS / Sass, Tailwind, Bulma</li>\n<li>JavaScript / ES6</li>\n</ul>\n<p><strong>Deployment</strong></p>\n<ul>\n<li>Docker, docker-compose</li>\n<li>AWS EC2, Google Cloud App Engine</li>\n<li>AWS S3, RDS (PostgreSQL)</li>\n<li>AWS Elastic Beanstalk, CodePipeline;</li>\n</ul>\n<style class=\"grvsc-styles\">\n  .grvsc-container {\n    overflow: auto;\n    position: relative;\n    -webkit-overflow-scrolling: touch;\n    padding-top: 1rem;\n    padding-top: var(--grvsc-padding-top, var(--grvsc-padding-v, 1rem));\n    padding-bottom: 1rem;\n    padding-bottom: var(--grvsc-padding-bottom, var(--grvsc-padding-v, 1rem));\n    border-radius: 8px;\n    border-radius: var(--grvsc-border-radius, 8px);\n    font-feature-settings: normal;\n    line-height: 1.4;\n  }\n  \n  .grvsc-code {\n    display: table;\n  }\n  \n  .grvsc-line {\n    display: table-row;\n    box-sizing: border-box;\n    width: 100%;\n    position: relative;\n  }\n  \n  .grvsc-line > * {\n    position: relative;\n  }\n  \n  .grvsc-gutter-pad {\n    display: table-cell;\n    padding-left: 0.75rem;\n    padding-left: calc(var(--grvsc-padding-left, var(--grvsc-padding-h, 1.5rem)) / 2);\n  }\n  \n  .grvsc-gutter {\n    display: table-cell;\n    -webkit-user-select: none;\n    -moz-user-select: none;\n    user-select: none;\n  }\n  \n  .grvsc-gutter::before {\n    content: attr(data-content);\n  }\n  \n  .grvsc-source {\n    display: table-cell;\n    padding-left: 1.5rem;\n    padding-left: var(--grvsc-padding-left, var(--grvsc-padding-h, 1.5rem));\n    padding-right: 1.5rem;\n    padding-right: var(--grvsc-padding-right, var(--grvsc-padding-h, 1.5rem));\n  }\n  \n  .grvsc-source:empty::after {\n    content: ' ';\n    -webkit-user-select: none;\n    -moz-user-select: none;\n    user-select: none;\n  }\n  \n  .grvsc-gutter + .grvsc-source {\n    padding-left: 0.75rem;\n    padding-left: calc(var(--grvsc-padding-left, var(--grvsc-padding-h, 1.5rem)) / 2);\n  }\n  \n  /* Line transformer styles */\n  \n  .grvsc-has-line-highlighting > .grvsc-code > .grvsc-line::before {\n    content: ' ';\n    position: absolute;\n    width: 100%;\n  }\n  \n  .grvsc-line-diff-add::before {\n    background-color: var(--grvsc-line-diff-add-background-color, rgba(0, 255, 60, 0.2));\n  }\n  \n  .grvsc-line-diff-del::before {\n    background-color: var(--grvsc-line-diff-del-background-color, rgba(255, 0, 20, 0.2));\n  }\n  \n  .grvsc-line-number {\n    padding: 0 2px;\n    text-align: right;\n    opacity: 0.7;\n  }\n  \n</style>","fields":{"slug":"/README/"},"frontmatter":{"date":"2021-09-25","title":"Portfolio","draft":true}}},{"node":{"excerpt":"","html":"<style class=\"grvsc-styles\">\n  .grvsc-container {\n    overflow: auto;\n    position: relative;\n    -webkit-overflow-scrolling: touch;\n    padding-top: 1rem;\n    padding-top: var(--grvsc-padding-top, var(--grvsc-padding-v, 1rem));\n    padding-bottom: 1rem;\n    padding-bottom: var(--grvsc-padding-bottom, var(--grvsc-padding-v, 1rem));\n    border-radius: 8px;\n    border-radius: var(--grvsc-border-radius, 8px);\n    font-feature-settings: normal;\n    line-height: 1.4;\n  }\n  \n  .grvsc-code {\n    display: table;\n  }\n  \n  .grvsc-line {\n    display: table-row;\n    box-sizing: border-box;\n    width: 100%;\n    position: relative;\n  }\n  \n  .grvsc-line > * {\n    position: relative;\n  }\n  \n  .grvsc-gutter-pad {\n    display: table-cell;\n    padding-left: 0.75rem;\n    padding-left: calc(var(--grvsc-padding-left, var(--grvsc-padding-h, 1.5rem)) / 2);\n  }\n  \n  .grvsc-gutter {\n    display: table-cell;\n    -webkit-user-select: none;\n    -moz-user-select: none;\n    user-select: none;\n  }\n  \n  .grvsc-gutter::before {\n    content: attr(data-content);\n  }\n  \n  .grvsc-source {\n    display: table-cell;\n    padding-left: 1.5rem;\n    padding-left: var(--grvsc-padding-left, var(--grvsc-padding-h, 1.5rem));\n    padding-right: 1.5rem;\n    padding-right: var(--grvsc-padding-right, var(--grvsc-padding-h, 1.5rem));\n  }\n  \n  .grvsc-source:empty::after {\n    content: ' ';\n    -webkit-user-select: none;\n    -moz-user-select: none;\n    user-select: none;\n  }\n  \n  .grvsc-gutter + .grvsc-source {\n    padding-left: 0.75rem;\n    padding-left: calc(var(--grvsc-padding-left, var(--grvsc-padding-h, 1.5rem)) / 2);\n  }\n  \n  /* Line transformer styles */\n  \n  .grvsc-has-line-highlighting > .grvsc-code > .grvsc-line::before {\n    content: ' ';\n    position: absolute;\n    width: 100%;\n  }\n  \n  .grvsc-line-diff-add::before {\n    background-color: var(--grvsc-line-diff-add-background-color, rgba(0, 255, 60, 0.2));\n  }\n  \n  .grvsc-line-diff-del::before {\n    background-color: var(--grvsc-line-diff-del-background-color, rgba(255, 0, 20, 0.2));\n  }\n  \n  .grvsc-line-number {\n    padding: 0 2px;\n    text-align: right;\n    opacity: 0.7;\n  }\n  \n</style>","fields":{"slug":"/draft-template/"},"frontmatter":{"date":"2020-02-13","title":"title","draft":true}}}]}},"pageContext":{}},"staticQueryHashes":[]}